所有接口中要求输入X_train和X_test的部分，输入的特征矩阵必须至少是一个二维矩阵。

==**sklearn不接受任何一维矩阵作为特征矩阵被输入**==。

如果你的数据的确只有一个特征，那必须用 `reshape(-1,1)` 来给矩阵增维；如果你的数据只有一个特征和一个样本，使用 `reshape(1,-1)` 来给你的数据增维。



## 决策树

### 概述

#### 决策树如何工作的

决策树（Decision Tree）是一种非参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。

决策树算法的本质是一种图结构，我们只需要问一系列问题就可以对数据进行分类了。

在这个决策过程中，我们一直在对记录的特征进行提问。最初的问题所在的地方叫做根节点，在得到结论前的每一个问题都是中间节点，而得到的每一个结论（动物的类别）都叫做叶子节点。

决策树算法的核心是要解决两个问题：

1. 如何从数据表中找出最佳节点和最佳分枝？
2. 如何让决策树停止生长，防止过拟合？

#### sklearn中的决策树

sklearn的基本建模流程

1. 实例化，建立评估模型对象
2. 通过模型接口训练模型
3. 通过模型接口提取需要的信息

在这个流程下，分类树对应的代码是：

```python
from sklearn import tree #导入需要的模块
clf = tree.DecisionTreeClassifier() #实例化
clf = clf.fit(X_train,y_train) #用训练集数据训练模型
result = clf.score(X_test,y_test) #导入测试集，从接口中调用需要的信息
```

决策树优点
1. 易于理解和解释，因为树木可以画出来被看见
2. 需要很少的数据准备。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等。但请注意，
sklearn中的决策树模块不支持对缺失值的处理。
3. 使用树的成本（比如说，在预测数据的时候）是用于训练树的数据点的数量的对数，相比于其他算法，这是
一个很低的成本。
4. 能够同时处理数字和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类
型的数据集。
5. 能够处理多输出问题，即含有多个标签的问题，注意与一个标签中含有多种标签分类的问题区别开
6. 是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松
解释条件。相反，在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。
7. 可以使用统计测试验证模型，这让我们可以考虑模型的可靠性。
8. 即使其假设在某种程度上违反了生成数据的真实模型，也能够表现良好

决策树的缺点
1. 决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据。这称为过度拟合。修剪，设置叶节点所
需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说
会比较晦涩
2. 决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树，这个问题需要通过集成算法来解决。
3. 决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法
不能保证返回全局最优决策树。这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过
程中被随机采样。
4. 有些概念很难学习，因为决策树不容易表达它们，例如XOR，奇偶校验或多路复用器问题。
5. 如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。因此，建议在拟合决策树之前平衡
数据集。





### 分类树

划分训练集和测试集

e.g. `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4,random_state=42)`



#### 重要参数

##### criterion

为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。

不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且**子节点的不纯度一定是低于父节点的**，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。Criterion这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择：

1. 输入”entropy“，使用信息熵（Entropy）
   当决策树的拟合程度不够的时候，使用信息熵。

2. 输入”gini“，使用基尼系数（Gini Impurity）

   数据维度很大，噪音很大时使用基尼系数，一般都使用基尼系数

比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是在实际使用中，**信息熵和基尼系数的效果基本相同**。

##### random_state & splitter

random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。

splitter也是用来控制决策树中的随机选项的，有两种输入值：

1. 输入”best"，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看）；
2. 输入“random"，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。

当你预测到你的模型会过合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合。

#### 剪枝参数调优⭐️

为了让决策树有更好的泛化性，我们要对决策树进行剪枝。剪枝策略对决策树的影响巨大，正确的剪枝策略是优化决策树算法的核心。sklearn为我们提供了不同的剪枝策略：

- max_depth

  限制树的最大深度，超过设定深度的树枝全部剪掉这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效果再决定是否增加设定深度。

- min_samples_leaf & min_samples_split

  min_samples_leaf 限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。

  一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。

  如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。

  min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。

- max_features & min_impurity_decrease

  一般max_depth使用，用作树的”精修“。max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。

  min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的功能，在0.19版本之前时使用min_impurity_split。

- 确认最优的剪枝参数

  那具体怎么来确定每个参数填写什么值呢？这时候，我们就要使用确定超参数的曲线来进行判断了，继续使用我们已经训练好的决策树模型clf。

  超参数的学习曲线，是一条以超参数的取值为横坐标，模型的度量指标为纵坐标的曲线，它是用来衡量不同超参数取值下模型的表现的线。在我们建好的决策树里，我们的模型度量指标就是score

#### 重要属性和接口

所有接口中要求输入X_train和X_test的部分，输入的特征矩阵必须至少是一个二维矩阵。

sklearn不接受任何一维矩阵作为特征矩阵被输入。

如果你的数据的确只有一个特征，那必须用 `reshape(-1,1)` 来给矩阵增维；如果你的数据只有一个特征和一个样本，使用 `reshape(1,-1)` 来给你的数据增维。

决策树最常用的接口还有apply和predict

- apply中输入测试集返回每个测试样本所在的叶子节点的索引

  ```python
  #apply返回每个测试样本所在的叶子节点的索引
  clf.apply(Xtest)
  ```

- predict输入测试集返回每个测试样本的标签

  ```python
  #predict返回每个测试样本的分类/回归结果
  clf.predict(Xtest)
  ```

#### 小结

sklearn自带的数据库中生成三种类型的数据集：1）月亮型数据，2）环形数据，3）二分型数据

分类树天生不擅长环形数据。每个模型都有自己的决策上限，所以一个怎样调整都无法提升表现的可能性也是有的。

当一个模型怎么调整都不行的时候，我们可以选择换其他的模型使用，不要在一棵树上吊死。

==**最擅长月亮型数据的是最近邻算法、RBF支持向量机和高斯过程；最擅长环形数据的是最近邻算法和高斯过程；最擅长对半分的数据的是朴素贝叶斯，神经网络和随机森林。**==

kNN算法的适用范围广，但是可调试范围小。



### 回归树 

几乎所有参数，属性及接口都和分类树一模一样。

需要注意的是，在回归树种，没有标签分布是否均衡的问题，因此没有class_weight这样的参数。

#### 参数

##### criterion

回归树衡量分枝质量的指标，支持的标准有三种：

1. 输入"mse"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。
2. 输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差。
3. 输入"mae"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心。

在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标。

当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。

值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。

这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质。**均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)**，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。

在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。

**R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正**。

**最好的衡量线性回归法的指标：R Squared** 

1. R^2 <= 1
2. R^2越大越好，当自己的预测模型不犯任何错误时：R2 = 1
3. 当我们的模型等于基准模型时：R^2 = 0
4. 如果R^2 < 0，说明学习到的模型还不如基准模型。 **#** **注：很可能数据不存在任何线性关系**



#### 属性和接口

#### 交叉验证

交叉验证是用来观察模型的稳定性的一种方法，我们将数据划分为n份，依次使用其中一份作为测试集，其他n-1份作为训练集，多次计算模型的精确性来评估模型的平均准确程度。训练集和测试集的划分会干扰模型的结果，因此用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量。

e.g.

`cross_val_score(regressor, boston.data, boston.target, cv=5,
scoring = "neg_mean_squared_error")` 

`boston.data, boston.target` 都来自原始数据集

 `scoring` 衡量模型的得分标准



### 回归树案例：用回归树拟合正弦曲线

如果树的最大深度（由max_depth参数控制）设置得太高，则决策树学习得太精细，它从训练数据中学了很多细节，包括噪声得呈现，从而使模型偏离真实的正弦曲线，形成过拟合。





### 决策树案例：泰坦尼克号生存者预测

















