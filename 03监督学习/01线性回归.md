# 广义线性模型——线性回归

## 辨析

[线性回归和逻辑回归](https://blog.csdn.net/jiaoyangwm/article/details/81139362)

## QA

### 如何将类别型变量引入线性回归，提出至少1种方案？

#### LabelEncoding 标签编码

作用是为变量的n个唯一取值分配一个[0,n-1]之间的编码，**将该变量转化成连续的数值型变量**。

LabelEncoding根据原各唯一取值的**先后顺序**进行排序后为其转化为对应次序的数值，转化后的结果也在数值上体现了这一优势。

- 相较于 Replace 等直接替换的方法，LabelEncoding 可以快速简便地将各取值转换成数值
- LabelEncoding 处理后的数值为取值范围[0, n-1]的整数数值，在计算空间距离等算法中存在优先级关系。因此 LabelEncoding **适合有序数据的处理**。
- LabelEncoding 处理后的取值，是没有具体含义的，数值不同只做区分作用，而不同于连续性数值一般具有现实意义（如在房屋售价回归模型中，房屋面积的增量可能导致售价的增量）
- 对于无序数据，如动物的种类，LabelEncoding 处理后的结果会使得无序数据存在有序性，这会扰乱原数据间的关系。因此 LabelEncoding 不适用于无序数据的处理

#### OneHot Encoding 独热编码

独热编码是**针对无序数据**常用的编码方案，对于原变量中存在的 n 个唯一的类别取值。

小结：独热编码将类别型数据转换成 n 个0-1变量，从而在数值上避免了这部分数据不应有的优先级问题。

优点：独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。

 缺点：当类别的数量很多时，特征空间会变得非常大，即庞大的稀疏矩阵。在这种情况下，一般可以用PCA来减少维度。而且**独热编码+PCA**这种组合在实际中也非常有用。

==独热编码同虚拟变量，在应用时认为是同一处理方法的不同表达==。

**Target Encoding 目标变量编码/均值编码**

Target Encoding就是用目标变量的类别均值来给类别特征做编码，该列中的每个值都被该类别的平均目标值替代。这可以更直接地表示分类变量和目标变量之间的关系。

CatBoost中就大量使用目标变量统计的方法来对类别特征编码。但在实际操作时，直接用类别均值替换类别特征的话，会造成一定程度的标签信息泄露的情况，主流方法是使用两层的交叉验证来计算目标均值。

Target Encoding一般适用于类别特征无序且类别取值数量大于5个的情形。

#### 虚拟变量 Dummy Variable 

定性变量成为指标变量，二元变量或分类变量，例如性别，年龄，宗教，民族，婚姻状况，教育程度等。

虚拟变量的作用：

分离异常因素的影响，检验不同属性类型对因变量的作用，提高模型精度，将不同属性的样本合并，扩大了样本容量(增加误差自由度，降低误差方差)

虚拟变量将类别型变量转化成n-1个0-1变量，在数值上避免了这部分数据不应有的优先级和虚拟变量陷阱问题。

在应用时，虚拟变量和独热编码一般认为是同一处理方法，一般情况下使用独热编码(n个0-1变量)；如果涉及线性回归问题，则需要转化虚拟变量处理(n-1个0-1 变量)

#### 针对高势集的解决方法

**主观合并类别**

根据相关业务背景、专业知识，对这些取值进行划分合并，手动将原始的 n 种取值降为 k 种取值（k远小于n）。

这样就可以把高数量类别特征转化成为低数量类别特征。随后，再对转化后的低数量类别特征应用OneHot 编码等处理，即可避免产生过于庞大的稀疏矩阵.

**Clustering**

将原始的 1-to-n 的 mapping 问题变成 1-to-k 的 mapping 问题（k远小于n）。

为了达到这个目标，这个高数量类别属性首先将依据 target 的值 grouping 成k个类(clusters)，然后再依据这个 grouping 的结果进行 one-hot 编码。Clustering侧重于使用相关算法及度量指标进行转化，最大化的保留了原始数据的信息。

**重编码——哈希编码**

- 哈希编码可以作为一种降维方法，实现简单，降低特征数量所需计算量小且效果好
- 哈希编码可以保持原有特征的稀疏性
- 哈希后学习到的模型很难检验，无法对模型参数做解释

#### 总结

**如何判断是定量变量还是定性变量**

- 定量变量。也就是通常所说的连续量，如长度、重量、产量、人口、速度和温度等，它们是由测量或计数、统计所得到的量，这些变量具有数值特征，称为定量变量。可以以某种方式排序，Age就是一个很好的列子。
- 定性变量。描述了物体的某一（不能被数学表示的）方面，Embarked就是一个例子。这些量并非真有数量上的变化，而只有性质上的差异。这些量还可以分为两种：
  1. 一种是有序变量，它没有数量关系，只有次序关系，如某种产品分为一等品、二等品、三等品等，矿石的质量分为贫矿和富矿。
  2. 另一种是名义变量，这种变量即无等级关系，也无数量关系，如天气（阴、晴），性别（男、女）、职业（工人、农民、教师、干部）和产品的型号等。

**定类**类型数据：

使用OneHotEncoding独热编码或者是TargetEncoding均值编码

定类类型就是纯分类，不排序，没有逻辑关系，比如性别分男女，男女之间不存在逻辑关系， 中国各省市分类也可以使用独热编码，各省间不存在逻辑关系。

如果转化为n-1个变量，即做虚拟变量。

**定序**类型数据：

使用LabelEncoding。**定序类型也是分类，但有排序逻辑关系，等级上高于定类**。例如，学历上小学初中高中本科研究生，各个类别之间存在一定的逻辑，显然研究生学历是最高的小学最低，此时使用LabelEncoding会更合适，因为自定义的数字顺序可以不破坏原有逻辑，并与这个逻辑相对应。

**数值大小敏感**的模型:

必须使用OneHotEncoding或者Dummy。典型例子就是LR和svm，二者的损失函数对数值大小是敏感的，并且变量间的数值大小是比较有意义的，而LabelEncoding的数字编码没有数值大小的含义，只有一种排序，因此这些模型都使用OneHotEncoding

**数值大小不敏感**的模型：

对数值大小不敏感的模型(例如树模型)最好不适用OneHotEncoding。

一般这类模型为树模型，将离散型特征进行OneHot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one_hot编码就可以很合理的计算出距离，那就没必要进行one_hot编码。有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。 

Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是增加树的深度。如果分类类别特别多，那么one-hot encoding会分裂出很多特征变量。这时候，如果我们限制了树模型的深度而不能向下分裂的话，一些特征变量可能就因为模型无法继续分裂而被舍弃损失掉了。因此，此种情况下可以考虑使用Label encoding。

另一种理解方式：

- label encoding

  特征存在内在顺序 (ordinal feature)

- one hot encoding

  特征无内在顺序，category数量 < 4

- target encoding (mean encoding, likelihood encoding, impact encoding)

  特征无内在顺序，category数量 > 4

- beta target encoding

  特征无内在顺序，category数量 > 4, K-fold cross validation

- 不做处理（模型自动编码）

  CatBoost，lightgbm

实际操作 https://zhuanlan.zhihu.com/p/85817133

### 自变量进行标准化会对模型带来哪些影响（梯度、回归系数等角度）？

数据的标准化(normalization)是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。 

常用的标准化有：Min-Max scaling, Z score

好处：

1. 提升模型的收敛速度，即梯度下降迭代速度
   自变量未标准化模型做梯度下降时，损失的等高线图呈窄长椭圆形状，而不是接近圆形，导致在梯度下降时梯度的方向为垂直等高线的方向而走之字路线。

2. 可以取消量纲的影响，使回归系数可以做比较。

   在多元线性回归中，通常各自变量取值的单位及其离散程度是不同的，所以量纲不同的各回归系数之间不能直接比较大小。

   为此，可先对所有的自变量和因变量进行标准化转换，即将变量减去其均值后再除以标准差。

   随后，以标准化变换后的因变量为因变量，以标准化后的各自变量为自变量，重新建立回归方程，得到的回归系数即标准化回归系数。

3. 提升模型的精度
   一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。
   如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）

4. 深度学习中数据归一化可以防止模型梯度爆炸

标准化的适用场景

像Adaboost、SVM、LR、Knn、KMeans、svm、线性回归之类的最优化问题就需要归一化。

什么时候不需要标准化

1. 当采用普通的**线性回归**的时候，是无需标准化的。因为标准化前后，不会影响线性回归预测值。
2. 同时，标准化不会影响logistic回归、决策树及其他一些集成学习算法，比如random forest and gradient boosting.
3. 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。
4. 树形模型不需要归一化。因为它们按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。









## 最小二乘（最小平方法）

- 最小二乘：基于经验的最小化误差平方根MSE；
- 误差服从正态分布是最小二乘的充要条件

$$\epsilon=\sum\left(y-y_{i}\right)^{2} \text { 最小 } \Rightarrow \text { 真值 } y$$

**算数平均数** 

这是一个二次函数，对其求导，导数为0的时候取得最小值：

$$\frac{d}{d y} \epsilon=\frac{d}{d y} \sum\left(y-y_{i}\right)^{2}=2 \sum\left(y-y_{i}\right)=2\left(\left(y-y_{1}\right)+\left(y-y_{2}\right)+\left(y-y_{3}\right)+\left(y-y_{4}\right)+\left(y-y_{5}\right)\right)=0$$

进而：

$$5 y=y_{1}+y_{2}+y_{3}+y_{4}+y_{5} \Rightarrow y=\frac{y_{1}+y_{2}+y_{3}+y_{4}+y_{5}}{5}$$

正好是算数平均数。

**正态分布** 

每次的测量值$$ x_{i} $$都和线段长度的真值$$x$$之间存在一个误差，$$\epsilon=x-x_{i}$$

这些误差最终会形成一个概率分布，只是现在不知道误差的概率分布是什么。假设概率密度函数为 $$
P(\epsilon)$$

再假设一个联合概率密度函数，这样方便把所有的测量数据利用起来：
$$
\begin{gathered}
L(x)=p\left(\epsilon_{1}\right) p\left(\epsilon_{2}\right) \ldots p\left(\epsilon_{5}\right) \\
=p\left(x-x_{1}\right) p\left(x-x_{2}\right) \ldots p\left(x-x_{5}\right)
\end{gathered}
$$
似然函数⬆️

因为$$L(x)$$是关于$$x$$的函数，并且也是一个概率密度函数，在下面这个式子成立时可以取得最大值
$$
\frac{d}{d x} L(x)=0
$$
最小二乘法给出的答案是：
$$
x=\bar{x}=\frac{x_{1}+x_{2}+x-3+x_{4}+x_{5}}{5}
$$
如果最小二乘法是对的，那么$$x=\bar{x}$$时应该取得最大值，即
$$
\left.\frac{d}{d x} L(x)\right|_{x=\bar{x}}=0
$$
解开最小二乘的微分方程，最终得到正态分布
$$
p(\epsilon)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{\epsilon^{2}}{2 \sigma^{2}}}
$$
并且这还是一个充要条件：
$$
x=\bar{x} \Leftrightarrow p(\epsilon)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{\epsilon^{2}}{2 \sigma^{2}}}
$$
也就是说，如果误差的分布是正态分布，那么最小二乘法得到的就是最有可能的值。 

那么误差的分布是正态分布吗？

我们相信，误差是由于随机的、无数的、独立的、多个因素造成的。

那么根据[中心极限定理](这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量近似服从正态分布的条件)，误差的分布就应该是正态分布。



## 最大似然估计

最大似然估计就是极大似然估计

**似然：** 英文单词为likelihood，有道翻译的翻译结果为：十有八九。

**概率：** 如果我有一枚质地均匀的硬币，那么它出现正面朝上的概率是0.5。

**似然：** 如果我抛一枚硬币100次，正面朝上52次，那么它十有八九是质地均匀的。



再举一个例子加深理解。 假设有人向我挑战一个“有利可图的赌博游戏”。

**概率：** 帮助我们计算预期的收益和损失(平均值、众数、中值、方差、信息比率、风险值、赌徒破产等等)。

**似然：** 帮助我们量化是否首先应该相信那些概率。

实际上，似然几乎可以等价于置信度。



## 什么是线性回归

定义：给定数据集D={(x1，y1)，(x2，y2)，…}，我们试图从此数据集中学习得到一个线性模型，这个模型尽可能准确地反应x()和y()的对应关系。这里的线性模型，就是属性(x)的线性组合的函数，可表示为：
$$
\begin{gathered}
f(x)=w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{d} x_{d}+b \\
f(x)=\mathbf{w}^{T} \mathbf{x}+b
\end{gathered}
$$
其中，W=(w1；W2；w3；...，wd)表示列向量。

这里w表示weight，权重的意思，表示对应的属性在预测结果的权重，这个很好理解，权重越大，对于结果的影响越大。那么通常的线性回归，就变成了如何求得变量参数的问题，根据求得的参数，我们可以对新的输入来计算预测的值（也可以用于对训练数据计算模型的准确度)。

通俗的理解：$$xi$$就是一个个的属性（例如西瓜的色泽，根蒂等），$$w/b$$，就是对应属性的参数（或者权重)，我们根据已有数据集来求得属性的参数（相当于求得函数的参数），然后根据模型来对于新的输入或者旧的输入来进行预测（或者评估）。



## 线性回归的似然函数

在满足残差服从正态分布的前提下，线性回归的参数估计、最小二乘估计和极大似然估计其实是等价的。



## 线性回归中的梯度下降法

梯度下降的定义

机器学习算法都需要最大化或最小化一个函数，这个函数被称为“目标函数“，其中我们一般把最小化的一类函数，称为“损失函数“。它能根据预测结果，衡量出模型预测能力的好坏。

在求损失函数最小化的过程中使用梯度下降法。





























