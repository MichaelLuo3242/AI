## 学习目标

- 了解奇异值分解的原理及推导过程
- 能在推荐算法中应用SVD解决问题

https://www.cnblogs.com/whatyouknow123/p/7515180.html

## 奇异值分解 SVD

### 回顾特征值和特征向量

首先回顾下特征值和特征向量的定义如下：
$$
A x=\lambda x
$$
其中A是一个nxn矩阵，x是一个n维向量，则λ是矩阵A的一个特征值，而x是矩阵A的特征值λ所对应的特征向量。

求出特征值和特征向量有什么好处呢？就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的个特征值$\lambda_{1} \leqslant \lambda_{2} \leqslant \ldots \leqslant \lambda_{3}$，以及这n个特征值所对应的特征向量$w_{1}) w_{2}) \ldots w_{n}$，那么矩阵A就可以用下式的特征分解表示：
$$
A=W \Sigma W^{-1}
$$
其中W是这n个特征向量所张成的nxn维矩阵，而Σ为这n个特征值为主对角线的nxn维矩阵。

一般我们会把W的这n 个特征向量标准化，即满足$\left\|w_{i}\right\|_{2}=1$，或者$w_{i}^{T} w_{i}=1$，此时W的n个特征向量为标准正交基，满足$W^{T} W=I$，即$W^{T}=W^{-1}$，也就是说$W$为酉矩阵（实正交矩阵）。这样我们的特征分解表达式可以写成：
$$
A=W \Sigma W^{T}
$$
注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。



### SVD的定义 

**SVD的全称是奇异值分解**，SVD的作用是它能够将高维的数据空间映射到低维的数据空间，实现**数据约减和去除噪声**的功能。

SVD的特点主要有以下几个方面：

- 优点：去除噪声，简化数据，提高算法的结果
- 缺点：数据的转化难以理解，分解出的矩阵解释性往往不强，有点黑盒子的味道，不过这不影响它的使用。
- 适用的数据：**数值型数据** 

SVD经常用于信息检索领域，在信息检索中我们将使用了SVD方法的数据文档数据处理方式称之为隐性语义索引。

隐性语义索引，它将一个文档分解为了词和词频，能够利用然后分解得到的矩阵进行奇异值分解从而或者文档的主题与概念。同时它还能够实现同义词的概念映射。

SVD也常常用于推荐系统，通过SVD对**数据约减**将原始数据进行空间映射，进行降维，得到的数据可以被定义为另一种概念，可以看做是一种概念抽象的过程。

SVD将一个矩阵($m*n$)分解成三个矩阵的乘积，第一个矩阵是$m*m$,第二个矩阵是$m*n$(它是一个对角矩阵，只有对角线的元素才可能为非零，并且它的值是按照从到排列的)，第三个矩阵是$n*n$;其中的第二个矩阵就是所说的奇异值。奇异值它能够在一定的程度上表现数据的特征，这一点和PCA中所说的特征值的概念很像，实际上奇异值和特征值也确实存在着联系。$Data*data^T$的特征值的开方就是data的奇异值。我们按照从大到小的顺序对奇异值进行排序，那么有可能出现奇异值为0的情况，那么这时候就说明该行的特征是多余的，我们在计算的时候就可以忽略它，这样就能够实现特征约减。

### SVD计算为例 

### SVD的一些性质 

### SVD用于PCA 

- 左奇异矩阵可以用于行数的压缩。
- 右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降维。

### SVD小结

SVD是一种去除噪声和降维的实用方法，我们可以通过SVD获取数据中的重要特征，通过保留矩阵百分之八十到百分之九十的能量，我们就可以保存重要的特征和去除噪声。SVD可以用于搜索引擎，它能够基于用户和行为数据进行协同过滤。

协同过滤的核心是相似度计算，相似度的度量方式有很多种，这里列举了常用的三种。

在大数据中应用SVD技术有点困难，我们可以使用离线处理的方式，在本地先进行SVD计算，然后再应用它。

- 第一种是1/(1+距离)
- 第二种是皮尔逊相关系数，但是皮尔逊相关系数的值是-1到1，要对它进行0到1的归一化，那么就变成了1/2+1/2*皮尔逊相关系数值。
- 第三种方式是余弦相似度。





## 奇异值分解(SVD)的应用

可以帮助你选择机器学习算法和模型。

做机器学习会发现，当你拿到一个新的数据集时，选择适合它的算法和参数是很难的，一般都是靠着直觉和经验去做。然而效果并不好，而客观上说我们也没有足够的资源去尝试所有的可能性。对于大部分机器学习实践者而言，选择模型是个无解题。

换个问题。如果你是一个电商平台，此时来了个新客户，你怎么给她推荐商品？

仔细一想后你会发现，为一个新的数据选择适合的模型和参数，就跟给一个新用户推荐商品是一个意思？其实都是「在冷启动环境下的协同过滤」。也就是你对新数据（用户）几乎一无所知的前提下如何选择模型（推荐商 品)。

那么这个时候，我们就可以用矩阵分解（奇异值分解可以来做矩阵分解）。

给定历史训练数据$P$，一个$n\times m$的矩阵，此处我们有$n$个训练数据以及$m$个可选择的模型，$Pij$代表第$j$个模型在第$i$个数据上的表现（比如准确度或者ROC)。之后对$P$进行矩阵分解$P=<U) V^{T}>$来学习$U$和$V$矩阵，分别代表数 据和模型在latent dimension(隐空间)上面的一些特点。

当一个新的数据集来的时候，你只需要先生成对应的$U'$，就可以通过$\left.P^{\prime}=<U^{\prime}\right) V^{T}>\in R^{1 \times m}$来得到新的数据集在$m$个模型上的表现的预测了。

此处还需要考虑一点，如何在有一个新数据集时得到对应的$U'$。现在比较主流的方法是训练一个回归，直接从数据本身回归到对应的U‘上去。比较常用的方法是对于所有的训练数据抽象为$n\times d$的元特征(meta-features)矩阵$M$，生成的过程包括统计数据的长度、维度、分布等，之后再把元特征回归到学习到的$U$上面去，可以用随机森林做个非线性的多元回归。

**为什么矩阵分解可以做模型选择？**

主要还是依靠分解过程去学习数据集本身的关联性，模型间的关联性，以及两者的联动。

矩阵分解的过程是对数据的压缩，因此可以得到更加精炼的表示并去除掉一些干扰和异常。如果我们再有一个方法能将任一数据转化到矩阵$U$上，就可以简单的通过点乘来获得个模型在新数据上的表现的预测，之后就可以选择最好的那个了。





## SVD案例分析——电影推荐系统

推荐系统是一种“信息过滤系统”，因为它们可以提高搜索结果的质量，并提供与搜索项更为相关或与用户的搜索历中相对应的内容。

推荐系统用于预测用户对某项商品的评价或偏好。几乎每家大型科技公司都以某种形式应用了它们：亚马逊使用它向客户推荐产品，YouTube使用它来决定自动播放下一个要播放的视频，Facebook使用它来推荐喜欢的页面和关注的人。此外，像Netfliⅸ和Spotify？这样的公司在很大程度上取决于其推荐系统对其业务和业务来源的有效性支持。

推荐系统基本有三种类型：

- 受众特征过滤

  根据电影的受欢迎程度和/或体裁，它们为每个用户提供通用建议。系统向具有相似受众统计特征的用户推荐相同的电影。由于每个用户都不相同，因此认为该方法太简单了。该系统背后的基本思想是，更受大众欢迎和好评的电影具有更高的被普通观众喜欢的可能性。

- 基于内容的过滤

  也就是根据”规定的那个项目“，来推荐”类似的项目“。该系统使用项目元数据(例如电影的流派，导演，描述，演员等)来提出这些建议。这些推荐系统背后的总体思想是，如果某人喜欢某个特定项目，那么他（她）也将喜欢与之相似的项目。

- 协同过滤

  此系统匹配具有相似兴趣的人，并根据此匹配提供推荐。协同过滤器不需要像“基于内容的推荐”那样的项目元数据。

### 加载数据集

### 受众特征过滤 

### 基于内容的过滤

#### 基于情节描述的推荐系统

转换每个overview的词向量：为每个概述计算词频-逆文件逆频率（TF-1DF) 向量。

##### 什么是 TF-IDF

TF-IDF(Term Frequency-Inverse Document Frequency， 词频-逆文件频率）是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

总结：一个词语在一篇文章中出现次数越多，同时在所有文档中出现次数越少，越能够代表该文章。这也就是TF-IDF的含义。

##### 什么是TF

TF(Term Frequency，词频)表示词条在文本中出现的频率，这个数字通常会被归一化(一般是词频除以文章总词数），以防止它向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。TF用公式表示如下
$$
T F_{i, j}=\frac{n_{i, j}}{\sum_{k} n_{k, j}}
$$
其中，$n_{ij}$表示词条$t_i$在文档$d_j$中出现的次数，$TF_{ij}$就是表示词条$t_i$，在文档$d_j$中出现的频率。

##### 什么是IDF

IDF(Inverse Document Frequency，逆文件频率)表示关键词的普遍程度。如果包含词条i的文档越少，IDF越大，则说明该词条具有很好的类别区分能力。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到
$$
\boldsymbol{I D F _ { i }}=\log \frac{|D|}{1+\left|j: t_{i} \in d_{j}\right|}
$$
其中，$|D|$表示所有文档的数量，$\left|j: t_{i} \in d_{j}\right|$表示包含词条$t_i$的文档数量，为什么这里要加$1$呢？主要是防止包含词条$t_i$的数量为0从而导致运算出错的现象发生。

某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语，表达为
$$
T F-I D F=T F \cdot I D F
$$
这会得出一个矩阵，其中的每一列代表overview中的一个单词（所有出现在至少一个文档中的单词），每一行代表一个电影，这样做是为了降低在情节描述中频繁出现的单词的重要性，也是它们在计算最终相似度得分中的重要性。

scikit-learn提供了一个内置的TfldfVectorizer类，该类以两行代码生成TF-IDF矩阵，非常好用。

```python
# 从 scikit-learn 导入 TfIdfvectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# 定义一个TF-IDE矢量化器对象。删除所有英语停用词，例如”the”, "a"
tfidf = TfidfVectorizer(stop_words='english')

# 用空字符串替换NaN
df2['overview'] = df2['overview'].fillna('')

# 通过拟合和转换数据来构造所需的TF-IDF矩阵
tfidf_matrix = tfidf.fit_transform(df2['overview'])

# 输出tfidf_matrix的shape
tfidf_matrix.shape
```



#### 基于品质，类型和关键字的推荐系统 

### 协同过滤

我们基于内容的过滤受到一些严重的限制。它仅能推荐接近某个电影的电影，也就是说，它无法捕捉口味并提供跨类型的推荐。

同时，我们构建的推荐系统不够真正的个性化，因为它无法捕获用户的个人品味和偏见。任何以电影为基础使用我们的推荐系统的人，都会获得与该电影相同的推荐。

所以接下来，将使用一种称为“协同过滤”的技术来向“电影观看者”提出建议。

它基本上有两种类型：

- 基于用户的过滤

  系统向用户的相似用户推荐用户喜欢的产品。为了测量两个用户之间的相似性，我们可以使用皮尔逊相关性或余弦相似性。该过滤方法可以用一个例子来说明，在以下矩阵中，每一行代表一个用户，而各列对应于不同的电影，但最后一个电影记录了该用户与目标用户之间的相似度。每个单元代表用户对该电影的评价。（假设用户E是目标用户）

  尽管userbased CF计算非常简单，但是它仍存在一些问题。一个主要问题是用户的喜好会随着时间而改变，这就表明基于相邻用户的预计算矩阵可能会导致性比较差的表现。为了解决这个问题，我们可以应用基于内容的协同过滤(item-based CF)。

- 基于内容的协同过滤

  item-based CF不会根据用户之间的相似度进行推荐，而是根据其与目标用户所评价内容的相似性来推荐内容。同样，可以使用皮尔逊相关性或余弦相似性来计算相似度。

  主要区别在于，与user-based CF的水平填写方式相反，对于item-based CF的协作过滤，我们竖着填写表格。

  由于item-based CF更静态，因此它成功避免了动态用户喜好带来的问题。但是，这种方法仍然存在一些问题。

  1. 首先，主要问题是**可扩展性**。计算量随着用户和内容一起增长。最坏的情况是复杂度为O(mn），其中m个用户n个内容。
  2. **稀疏性**是另一个问题。尽管只有一个用户同时评价了Matrix和Titanic，但它们之间的相似度已经达到了1。在极端情况下，我们可以拥有数百万个用户，而两部完全不同的电影之间的相似度可能很高，可能就是因为一个用户给两者的评分相似。

**基于协同过滤的推荐引擎**

所谓的协同过滤就是利用用户和其他用户进行对比实现推荐。这个对比我们就需要用到相似度的概念，比如两个人喜欢的电影的相似度很高，那么其中一个人看过的电影就可以推荐给另一个没看过该电影的人看。

在计算相似度的时候，我们将行向量之间的相似度称之为用户相似度，将列向量之间的额相似度称之为物品相似度，具体使用用户相似度还是物品相似度取决于那个的数量更少。对于一般的推荐系统来说，商品的数量会明显的更少，所以我们倾向于选择使用商品相似度。

#### 单值分解

处理CF创建的可伸缩性和稀疏性问题的一种方法是利用“潜在因子模型“来捕获用户和项目之间的相似性。本质上，我们希望将推荐问题转化为优化问题，可以将其视为预测给定用户物品评分的准确度。一种常见的度量方式是均方根误差（RMSE)。**RMSE越低，性能越好**。

现在谈论”潜在因子“时，你可能想知道这是什么？这是一个广义的概念，它描述了用户或物品具有的属性或概念。

例如，对于音乐，潜在因子可以指音乐所属的流派。SVD通过提取其潜在因子来减小效用矩阵的维数。本质上，我们将每个用户和每个项目映射到维度为r的潜在空间中。因此，当它们直接可比时，它可以帮助我们更好地了解用户与物品之间的关系。

#### 总结

我们使用受众特征过滤，基于内容的过滤和协同过滤来创建推荐系统。虽然受众特征过滤非常简单，但却无法实际使用。混合系统可以利用基于内容的过滤和协同过滤，因为事实证明这两种方法几乎是互补的。该模型是非常基础的参照模型，仅提供了一个入门的框架。






























